{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Третья лабораторная. Методы машинного обучения\n",
    "\n",
    "Лабораторная состоит из гайда по методам глубинного обучения (Deep Learning) и \"продвинутым\" методам машинного обучения (Machine Learning) на Python и пяти заданий:\n",
    "\n",
    "* [Задание 1](#Задание-1.) - поиск гиперпараметров случайного леса\n",
    "\n",
    "* [Задание 2](#Задание-2.) - поиск гиперпараметров нейросети для классификации\n",
    "\n",
    "* [Задание 3](#Задание-3.) - поиск гиперпараметров для градиентного бустинга\n",
    "\n",
    "* [Задание 4 (дополнительное)](#Задание-4-(дополнительное).) - выбор подходящих аугментаций. Это задание повышенной сложности, за него даётся 3 балла\n",
    "\n",
    "* [Задание 5 (дополнительное)](#Задание-5-(дополнительное).) - выбор модели машинного обучения для решения задачи регрессии. За это задание можно получить 1 балл\n",
    "\n",
    "Из пяти заданий требуется выполнить минимум первые 3 задания, за остальные задания можно заработать дополнительные баллы.\n",
    "\n",
    "Задания 2, 3 и 4 последовательно связаны друг с другом - для одной задачи требуется добиться необходимого качества разными способами. Чтобы выполнить задание 3, требуется сначала выполнить задание 2, так как там обучается нейросеть для предобработки (имейте это в виду, если будете перезапускать ноутбук)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*В этом ноутбуке изначально опущены результаты исполнения кода. Рекомендуется запускать (Shift+Enter) ячейки по мере просмотра документа*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Введение\n",
    "\n",
    "Центральное понятие в машинном обучении - **модель**. Задачей машинного обучения является создание (обучение) модели, максимально <u>качественно</u> описывающей обучающие <u>данные</u>. Цель - получить модель, способную делать полезные предсказания по новым данным.\n",
    "\n",
    "При этом критерии <u>качества</u> могут быть различными, и сильно влияют на результирующую модель. Определить *идеальный критерий*, подходящий для любого случая сложно (или невозможно). Это обусловлено как минимум двумя проблемами:\n",
    "* Данные частично некорректны - содержат шум и выбросы.\n",
    "    Шум возникает, например, из-за неточности измерений, а выбросы - чаще из-за ошибок; \n",
    "* Данные могут быть неполны - не полностью отражать действительность.\n",
    "\n",
    "Для получения полезной с практической точки зрения модели, надо учитывать, что обучающие данные (training set) могут отличаться от данных, на которых требуется вычислять предсказания (evaluation / test set). \n",
    "\n",
    "\n",
    "Как правило задачи машинного обучения разделяют на две категории:\n",
    "\n",
    "**1. [Обучение без учителя](https://scikit-learn.org/stable/unsupervised_learning.html)**\n",
    "\n",
    "* [Кластеризация](https://scikit-learn.org/stable/modules/clustering.html);\n",
    "* [Поиск ассоциативных правил](https://ru.wikipedia.org/wiki/Обучение_ассоциативным_правилам);\n",
    "* [Понижение размерности](https://scikit-learn.org/stable/modules/unsupervised_reduction.html);\n",
    "* [Поиск выбросов](https://scikit-learn.org/stable/modules/outlier_detection.html).\n",
    "\n",
    "**2. [Обучение с учителем](https://scikit-learn.org/stable/supervised_learning.html)**\n",
    "\n",
    "* [Классификация](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html);\n",
    "* Регрессия;\n",
    "* Ранжирование;\n",
    "* [Анализ выживаемости / рисков](https://nbviewer.jupyter.org/github/sebp/scikit-survival/blob/master/examples/00-introduction.ipynb).\n",
    "\n",
    "Но также есть множество задач, не вписывающихся в эти категории, например:\n",
    "\n",
    "* Multiple-instance Learning - целевые значения сопоставлены группам (bag) наблюдений;\n",
    "* One-shot Learning - классификация с одним обучающим примером для каждого класса;\n",
    "* Similarity Learning - поиск меры схожести объектов;\n",
    "* Reinforcement Learning - обучение модели, совершающей действия в ходе взаимодействия со средой;\n",
    "* Collaborative Filtering - построение рекомендаций;\n",
    "* Natural Language Processing: машинный перевод, QA - получение текстового ответа на текстовый вопрос;\n",
    "* Перенос стиля, генерирование изображений.\n",
    "\n",
    "Поэтому перед созданием модели лучше не сразу пытаться свести задачу к базовым задачам машинного обучения (классификации, регрессии), а поискать модели, использующие специфику области. Например, для машинного перевода специфичен порядок слов в предложении, и правильных переводов может быть несколько - решение задачи без использования этих особенностей приводит к крайне плохим результатам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting / Underfitting\n",
    "\n",
    "Чтобы полученная модель была практически применима, надо соблюсти баланс между переобучением (overfitting) и недообучением (underfitting):\n",
    "* переобучение возникает, когда модель имеет минимальную ошибку на обучающем множестве, и значительно большую на тестовом;\n",
    "* недообучение возникает при использовании слишком простых моделей, которые не могут обеспечить достаточно маленькую среднюю ошибку даже на тренировочных данных.\n",
    "\n",
    "В качестве примера рассмотрим задачу одномерной регрессии с синтетическими данными:\n",
    "\n",
    "Обучающее множество: $\\{(x_i, y_i)\\}_{i=1}^{n}$,\n",
    "где $x_i$ - наблюдения $x \\sim U([0; 1]); \\varepsilon \\sim N(0, 0.1)$, \n",
    "$y = f(x) + \\varepsilon$,\n",
    "\n",
    "$f(x) = \\cos(5 x + \\frac{\\pi}{2})$\n",
    "\n",
    "Для простоты будем использовать полиномиальную модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(239)\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(x * 5 + np.pi/2)\n",
    "\n",
    "X = np.sort(np.random.rand(20))\n",
    "y = f(X) + np.random.normal(0, 0.1, len(X))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "X_lin = np.linspace(0, 1, 50)  # массив 50 чисел от 0 до 1\n",
    "titles = [\"Underfit. Degree = 2\", \"Best fit. Degree = 4\", \"Overfit. Degree = 12\"]\n",
    "for i, title in enumerate(titles):\n",
    "    ax[i].set_title(title)\n",
    "    ax[i].scatter(X, y)  # наблюдения\n",
    "    ax[i].plot(X_lin, f(X_lin), color='g')  # целевая функция\n",
    "# модель - полином, подбор коэффициентов - МНК\n",
    "predictor_underfit = np.poly1d(np.polyfit(X, y, deg=2))\n",
    "ax[0].plot(X_lin, predictor_underfit(X_lin))\n",
    "\n",
    "predictor_best = np.poly1d(np.polyfit(X, y, deg=4))\n",
    "ax[1].plot(X_lin, predictor_best(X_lin))\n",
    "ax[1].plot(X_lin, f(X_lin), color='g')\n",
    "\n",
    "predictor_overfit = np.poly1d(np.polyfit(X, y, deg=12))\n",
    "ax[2].plot(X_lin, predictor_overfit(X_lin))\n",
    "ax[2].plot(X_lin, f(X_lin), color='g')\n",
    "\n",
    "ax[0].legend([\"f(x)\", \"Prediction\", \"Observation\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что третий вариант аппроксимации функции (с переобучением) очевидно не может быть использован на практике. Как минимум, из-за слишком больших значений в промежутке от 0 до 0.1.\n",
    "\n",
    "Однако о первом варианте (с недообучением) такого сказать нельзя. Хотя полученная функция не совпадает с $f(x)$, она достаточно неплохо оценивает обучающее множество. На практике при малом числе наблюдений и большой размерности пространства сложные модели скорей всего будут склонны к переобучению, тогда как простые, вроде линейной регрессии, будут давать хоть насколько-то верную аппрокимацию.\n",
    "\n",
    "Посмотрим на ошибки в данном примере на обучающем множестве:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def print_err(title, model, x, y_true):\n",
    "    mse = mean_squared_error(y_true, model(x))\n",
    "    mae = mean_absolute_error(y_true, model(x))\n",
    "    r2  = r2_score(y_true, model(x))\n",
    "    print(title)\n",
    "    print(f\"MSE: {mse:.3f}, MAE: {mae:.3f}, R^2: {r2:.3f}\\n\")\n",
    "\n",
    "for i, model in enumerate([predictor_underfit, predictor_best, predictor_overfit]):\n",
    "    print_err(titles[i], model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь $R^2$ - это коэффициент детерминации. Его множество значений от $-\\infty$ до $1$.\n",
    "\n",
    "Эти значения ошибок показывают насколько хорошо модель смогла запомнить обучающие данные.\n",
    "\n",
    "Попробуем применить модель к новым данным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.sort(np.random.rand(200))\n",
    "y_test = f(X_test) + np.random.normal(0, 0.1, len(X_test))\n",
    "\n",
    "for i, model in enumerate([predictor_underfit, predictor_best, predictor_overfit]):\n",
    "    print_err(titles[i], model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что качество предсказания первых двух моделей почти не упало, когда как третья (переобученная) даёт ошибку на два порядка больше. Таким образом можно выбрать наиболее корректную модель.\n",
    "\n",
    "Формально, разность между риском (математическим ожиданием ошибки) и эмпирическим риском (средним значением ошибки на обучающем множестве), называется **переобученностью**.\n",
    "\n",
    "В примере мы смогли **сгенерировать** новый набор данных, на котором проверяли насколько хорошо подходит модель, **приближая** значение переобученности (вместо математического ожидания ошибки мы вычисляли среднюю, но **не** на обучающем множестве). На практике сгенерировать новый набор данных может быть трудоёмко, или вообще невозможно, поэтому нужно уметь оценивать модель, имея только один (тренировочный) набор данных. Но как?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация (Cross-Validation)\n",
    "\n",
    "Для того, чтобы понять насколько хорошо подходит модель, воспользуемся [перекрёстной проверкой](https://ru.wikipedia.org/wiki/Перекрёстная_проверка) на имеющихся обучающих данных: будем обучать модель $k$ раз, каждый раз выделяя $k-1$ долей обучающего множества для обучения модели (с нуля), используя оставшуюся долю для оценки качества. Это позволит оценить не только качество модели, но и чувствительность к обучающим данным, причём численно.\n",
    "\n",
    "Часто модели имеют гиперпараметры (параметры, значения которых фиксируются до начала обучения, например количество слоёв в нейронной сети). В примере выше гиперпараметр - это степень полинома. Подберём оптимальные гиперпараметры с помощью C-V. Для этого можно воспользоваться одной из следующих готовых реализаций:\n",
    "* [Grid-search C-V](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) - перебор значений гиперпараметров на некоторой сетке;\n",
    "* [Randomized search C-V](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) - перебор случайных значений гиперпараметров;\n",
    "* [Bayesian C-V](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html) - перебор значений гиперпараметров, используя модифицируемое по ходу представление о форме поверхности ошибки. Реализован в библиотеке [hyperopt](https://hyperopt.github.io/hyperopt/).\n",
    "\n",
    "Наиболее популярный на данный момент метод - Bayesian C-V, но для простоты рассмотрим классический Grid-search C-V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# полиномиальный регрессор - то же самое, что линейная регрессия с признаками (x^0, x^1, x^2, ..., x^k)\n",
    "polynomial_regressor = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "# сетка параметров степени полинома - целые значения от 0 до 14:\n",
    "parameters = {'polynomialfeatures__degree': range(15)}\n",
    "\n",
    "# scoring - минимизируемая метрика: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "search = GridSearchCV(polynomial_regressor, parameters, cv=5, scoring='neg_mean_squared_error')\n",
    "search.fit(X[:, np.newaxis], y)\n",
    "print(f\"Best loss (-MSE): {search.best_score_}\")\n",
    "print(f\"Best parameters: {search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблему переобучения также можно решать с помощью регуляризации (например, в случае линейных моделей – Lasso, Ridge). При этом возникают дополнительные параметры, которые имеет смысл также перебирать с помощью кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Продвинутые методы машинного обучения\n",
    "\n",
    "### Ансамблевые методы\n",
    "\n",
    "Основная идея ансамблевых методов заключается в построении набора различных примитивных базовых моделей и использовании их комбинации для предсказаний.\n",
    "\n",
    "Это позволяет бороться с двумя проблемами:\n",
    "1. Переобучение - за счёт различия моделей (структуры или данных, на которых модели обучаются);\n",
    "2. Низкое качество предсказаний - для большинства моделей за счёт т.н. \"мудрости толпы\".\n",
    "\n",
    "В качестве базовых моделей могут использоваться любые методы, от линейной регрессии до нейронных сетей, однако стоит учитывать, что:\n",
    "* время построения ансамбля сложных моделей будет очень велико. Простые же модели можно строить параллельно;\n",
    "* на [bias-variance trade-off](https://habr.com/ru/company/ods/blog/323890/#razlozhenie-oshibki-na-smeschenie-i-razbros-bias-variance-decomposition) влияет сложность каждой базовой модели и их количество. Наиболее сложные базовые модели не всегда позволяют получать лучшее качество.\n",
    "\n",
    "#### 1. Случайный лес\n",
    "\n",
    "Случайный лес - ансамбль решающих деревьев (построенных по CART). Финальный прогноз производится усреднением результатов работы каждого дерева.\n",
    "\n",
    "Для получения различных деревьев используются два принципа:\n",
    "* Bagging (Bootstrap aggregation) - выборка для каждого дерева формируется случайным образом (с возвратом) из тренировочной. Методы, использующие bagging всегда могут оценивать ошибку на \"out of bag\" элементах начальной выборки - тех, что не вошли ни в одну из подвыборок;\n",
    "* Random feature subspace - в качестве признаков каждое дерево рассматривает случайное подмножество исходных признаков.\n",
    "\n",
    "Одним из основных преимуществ случайных лесов для классификации является возможность предсказать не только класс, но и \"уверенность\" - аппроксимацию вероятности данного класса при условии входных данных. Также они могут применяться для подбора наиболее релевантных признаков - т.н. feature importances.\n",
    "\n",
    "Рассмотрим простую задачу регрессии Boston House Prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все признаки в этом датасете численные. Требуется научиться предсказывать цену дома по его параметрам.\n",
    "\n",
    "Взглянем на датасет. Понизим размерность с помощью [T-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) и [Spectral Embedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html), и поставим цвет, соответствующий значению $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE, SpectralEmbedding\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "boston_X, boston_y = boston\n",
    "\n",
    "xy = []\n",
    "xy.append(TSNE(n_components=2).fit_transform(boston_X))\n",
    "xy.append(SpectralEmbedding(n_components=2).fit_transform(boston_X))\n",
    "xy.append(PCA(n_components=2).fit_transform(boston_X))\n",
    "\n",
    "titles = ['T-SNE', 'Spectral Embedding', 'PCA']\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, data in enumerate(xy):\n",
    "    cx = ax[i].scatter(data[:, 0], data[:, 1], c=boston_y, cmap='rainbow')\n",
    "    ax[i].set_title(titles[i])\n",
    "omit = fig.colorbar(cx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Задание 1.\n",
    "\n",
    "В коде ниже приведён пример запуска `RandomForestRegressor`.\n",
    "\n",
    "Требуется подобрать параметры (кроме `random_state` и `oob_score`) у `RandomForestRegressor`, чтобы [коэффициент детерминации $R^2$](http://www.machinelearning.ru/wiki/index.php?title=Коэффициент_детерминации) (оценка качетва) увеличился на **тестовых данных**, желательно до $0.85$.\n",
    "\n",
    "_В этом задании и последующих рекомендуется использовать автоматический подбор гиперпараметров вместо того, чтобы подбирать гиперпараметры вручную после каждого обучения модели, которая не смогла достичь требуемого качества._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def model_score(X, y, estimator=None, cv=3, scoring=None):\n",
    "    scores = cross_val_score(estimator, X, y, cv=cv, scoring=scoring)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston_X, boston_y, test_size=0.25, random_state=12345)\n",
    "\n",
    "# Ваши значения параметров:\n",
    "boston_rf = RandomForestRegressor(n_estimators=20,\n",
    "                                  max_depth=3,\n",
    "                                  max_features='auto',\n",
    "                                  min_samples_split=2,\n",
    "                                  min_samples_leaf=1,\n",
    "                                  criterion='mse',\n",
    "                                  \n",
    "                                  random_state=12345,  # `random_state` оставьте зафиксированным, чтобы можно было\n",
    "                                                       # повторить ваш результат\n",
    "                                  oob_score=True)\n",
    "boston_rf.fit(X_train, y_train)\n",
    "\n",
    "cv_score = model_score(X_train, y_train, estimator=boston_rf, cv=10, scoring='r2')\n",
    "train_score = boston_rf.score(X_train, y_train)\n",
    "test_score = boston_rf.score(X_test, y_test)\n",
    "oob_score = boston_rf.oob_score_\n",
    "print(\"Score (R^2) on train:\")\n",
    "print(f\"Out of bag training score estimation: {oob_score}\")\n",
    "print(f\"Cross-validation on train score: {cv_score}\")\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ещё одной интересной особенностью ансамблей деревьев является возможность подсчитать \"*важность*\" признаков. Сделаем это для датасета boston:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importances = boston_rf.feature_importances_\n",
    "\n",
    "# достанем названия признаков:\n",
    "boston_annotation = load_boston()\n",
    "feature_names = boston_annotation.feature_names\n",
    "# описания признаков:\n",
    "descriptions = [val for val in boston_annotation.DESCR.split(\"\\n\")\n",
    "                    if val.find(\"        -\") >= 0]\n",
    "feature_descriptions = {feature: feature.join(desc.split(feature)[1:])\n",
    "                        for desc in descriptions\n",
    "                        for feature in feature_names if desc.find(feature) >= 0}\n",
    "\n",
    "ind = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(feature_names[ind], importances[ind])\n",
    "plt.title(\"Feature importances\")\n",
    "\n",
    "top_features = list(feature_names[ind][:-4:-1])\n",
    "print(\"Топ 3 важных признаков:\")\n",
    "print(\"\\n\".join([feature + feature_descriptions[feature] for feature in top_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо того, что *важность* признаков сама по себе может быть полезной информацией для конечного пользователя, на основе неё можно производить автоматический отбор признаков.\n",
    "\n",
    "Однако следует быть осторожным с использованием такого механизма. Жадная природа алгоритма построения дерева решений может привести к неадекатным значениям такой *важности* признаков. Более надёжных методом (но тоже не идеальным!), подходящим для любых моделей, а не только для лесов, является, например, *permutation importance*.\n",
    "\n",
    "На практике помимо базового Random Forest также применяют модификации:\n",
    "* Extra Random Trees - деревья строятся не по алгоритму CART, вместо этого каждое правило деления (split point) выбирается случайным образом, после чего выбирается наиболее релевантный признак. Получается быстрое время построения, но не минимальная ошибка;\n",
    "* Rotation Random Forest - перед построением каждого дерева к набору предназначенных ему признаков применяется PCA (что означает поворот осей). Такой метод полезно использовать при больших размерностях пространства признаков.\n",
    "\n",
    "#### 2. [Градиентный бустинг](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "\n",
    "Градиентный бустинг тоже является ансамблевым методом и имеет следующие особенности:\n",
    "* деревья строятся последовательно - каждая следующая модель старается улучшить ошибку предыдущей, аппроксимируя антиградиент <u>функции потерь</u>;\n",
    "* результат работы считается как взвешенная сумма результатов базовых моделей, веса которой определяются линейным поиском на этапе построения ансамбля;\n",
    "* в случае классификации, в сравнении со случайными лесами, градиентный бустинг не позволяет хранить в листьях деревьев классы напрямую. Вместо этого алгоритм находит вероятности классов, как в логистической регрессии. Попробуйте ответить почему это так?\n",
    "\n",
    "Модели, опирающиеся на градиентный бустинг часто дают наиболее высокое качество, в сравнении со случайными лесами, линейной регрессией, SVM и т.д.\n",
    "Однако используя градиентный бустинг гораздо легче прийти к переобучению.\n",
    "\n",
    "Одним из способов борьбы с переобучением является введение скорости обучения `learning rate` - константного множителя для каждой новой базовой модели.\n",
    "\n",
    "Посмотрим как градиентный бустинг справляется с задачей регрессии на том же датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "boston_gbm = GradientBoostingRegressor(loss='ls',\n",
    "                                       learning_rate=0.1,\n",
    "                                       n_estimators=100,\n",
    "                                       subsample=.5,  # стохастический бустинг - деревья строятся по случайным подвыборкам\n",
    "                                       criterion='friedman_mse',\n",
    "                                       max_depth=3,\n",
    "                                       max_features='auto',\n",
    "                                       random_state=12345)\n",
    "boston_gbm.fit(X_train, y_train)\n",
    "\n",
    "cv_score = model_score(X_train, y_train, estimator=boston_gbm, cv=10, scoring='r2')\n",
    "train_score = boston_gbm.score(X_train, y_train)\n",
    "test_score = boston_gbm.score(X_test, y_test)\n",
    "\n",
    "print(\"Score (R^2) on train:\")\n",
    "print(f\"Cross-validation on train score: {cv_score}\")\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике градиентный бустинг из библиотеки sklearn обычно не применяют. Вместо него используют одну из трёх(+) библиотек, в каждой из которых реализованы свои усовершенствования оригинального алгоритма:\n",
    "* [XGBoost](https://xgboost.readthedocs.io/en/latest/) - первая библиотека, реализующая градиентный бустинг на GPU (2014 год);\n",
    "* [LightGBM](https://lightgbm.readthedocs.io/en/latest/) - библиотека от Microsoft Research (2017 год), в которой реализована обработка категориальных признаков и усовершенствования производительности;\n",
    "* **[CatBoost](https://catboost.ai/)** - библиотека от Yandex (2017 год), в которой реализована продвинутая обработка категориальных признаков и методы борьбы с переобучением. Чаще всего в задачах с категориальными признаками CatBoost работает лучше других алгоритмов даже без подстройки параметров, однако чтобы ощутить преимущество следует не применять препроцессинг для категориальных признаков. По словам команды разработчиков, использованный метод построения симметричных деревьев (Oblivious Trees) даёт прирост в качестве во всех случаях. Хотя определяющий фактор для применения таких деревьев – эффективность реализации на GPU.\n",
    "\n",
    "Установим CatBoost и проверим смелое утверждение разработчиков, не меняя ни одного параметра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "boston_catboost = CatBoostRegressor(verbose=False,\n",
    "                                    allow_writing_files=False,  # не пишем логи, о них ниже\n",
    "                                    task_type='CPU')  # или 'GPU'\n",
    "boston_catboost.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "\n",
    "cv_score = model_score(X_train, y_train, estimator=boston_catboost, cv=10, scoring='r2')\n",
    "train_score = r2_score(y_train, boston_catboost.predict(X_train))\n",
    "test_score = r2_score(y_test, boston_catboost.predict(X_test))\n",
    "\n",
    "print(\"Score (R^2) on train:\")\n",
    "print(f\"Cross-validation on train score: {cv_score}\")\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуализация\n",
    "\n",
    "При обучении моделей градиентного бустинга или сетей очень полезно строить графики функции потерь для тренировочной и тестовой выборок.\n",
    "\n",
    "*CatBoost* сохраняет логи обучения по умолчанию (если `allow_writing_files == True`) в формат, совместимый с *TensorBoard* и *CatBoost Viewer*. Использовать *TensorBoard* для просмотра логов крайне удобно: достаточно установить *TensorBoard* (`pip install tensorboard`) и запустить его командой `tensorboard --logdir=catboost_info`. Сохранять результаты запусков алгоритма с различными параметрами можно с помощью записи логов в подкаталоги с уникальными именами. С установкой *CatBoostViewer* могут возникнуть небольшие сложности, поскольку он реализован с использованием *NodeJS*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронные сети\n",
    "\n",
    "Если вы планируете использовать нейронные сети, имеет смысл ознакомиться с [этим](https://github.com/abidlabs/AtomsOfDeepLearning/blob/master/Atomic%20Experiments%20in%20Deep%20Learning.ipynb) ноутбуком.\n",
    "\n",
    "С нейронными сетями чаще всего гораздо сложнее работать, чем с градиентным бустингом: приходится подбирать архитектуру, настраивать огромное число гиперпараметров; нет возможности узнать по какому принципу сеть находит ответ. Однако есть ситуации, в которых нейронные сети просто не имеют адекватно работающих альтернатив:\n",
    "* обработка изображений, видео - Convolutional NN;\n",
    "* построение эмбеддинга с учителем и без учителя - AutoEncoder, Siamese NN;\n",
    "* работа с последовательностями, в том числе генерирование - Recurrent NN и **Transformer**;\n",
    "* создание генеративных моделей без учителя - Generative Adversarial Networks, Noise Conditional Score Network, и т.д.\n",
    "\n",
    "Большая часть преимуществ достигается из-за возможности точно управлять целью обучения с помощью функции потерь, а также возможности использовать части обученной сети для решения новой задачи. Например, научив сеть предсказывать корректность словосочетания, можно применять найденный сетью способ кодирования слов для нахождения семантической близости.\n",
    "\n",
    "#### Кратко о фреймворках\n",
    "\n",
    "Есть несколько актуальных на данный момент фреймворков для работы с нейронными сетями:\n",
    "* TensorFlow - построение статических графов вычислений, автоматический вывод градиентов, блоки для построения сетей. Имеет высокий \"порог вхождения\". С версии `2.0` поддерживает т.н. eager-вычисления, т.е. динамические графы. Обратная совместимость с версиями `1.*` частично отсутствует и может потреобвать переписывания части кода;\n",
    "* PyTorch - построение динамических графов вычислений, лёгкий перенос вычислений на GPU (требует изменения кода);\n",
    "* Keras - построение нейронных сетей из блоков. Вычисления производятся с помощью одного из низкоуровневых фреймворков (backend): TensorFlow, CNTK, Theano и др.\n",
    "\n",
    "Если не требуется реализовывать принципиально новый алгоритм машинного обучения, внедрять наиболее современные механизмы и т.д., проще всего использовать Keras.\n",
    "\n",
    "Для некоторых архитектур нейронных сетей пишут профильные оптимизированные решения (например, для [YOLO](https://pjreddie.com/darknet/yolo/) - алгоритма быстрого поиска объектов на изображениях реализована низкоуровневая поддержка в библиотеке [Darknet](https://github.com/pjreddie/darknet)). Фреймворки вроде TensorFlow, хоть и обеспечивающие работу с несколькими GPU в кластерах, часто проигрывают по производительности узкопрофильным решениям. На практике, если применяют TensorFlow, как правило используют [оптимизаторы](https://developer.nvidia.com/tensorrt) для ускорения инференса, а для этапа тренировки тратят приличное число ресурсов.\n",
    "\n",
    "\n",
    "#### Пример\n",
    "\n",
    "Установим Keras и backend к нему:\n",
    "\n",
    "**Если у Вас есть GPU с поддержкой CUDA, имеет смысл установить Tensorflow-GPU через conda:\n",
    "`conda install tensorflow-gpu`** (в таком случае не придётся вручную устанавливать CUDA SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "#! conda install tensorflow-gpu -y\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим сеть для решения задачи регрессии, рассмотренной выше.\n",
    "\n",
    "В Keras удобно строить сеть как последовательность слоёв, для этого используется *Sequential*-модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def make_dense_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    # чтобы модель можно было тренировать, необходимо её скомпилировать\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "dense_model = make_dense_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем натренировать модель примерно до того же качества, что и градиентный бустинг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.random import set_seed as set_random_seed\n",
    "np.random.seed(1)\n",
    "set_random_seed(1)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "history = dense_model.fit(X_train, y_train,\n",
    "                          epochs=n_epochs,\n",
    "                          # число примеров, на которых считается градиент:\n",
    "                          batch_size=16,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          verbose=False)\n",
    "\n",
    "plt.plot(range(n_epochs), (history.history['loss']), c='b')\n",
    "plt.plot(range(n_epochs), (history.history['val_loss']), c='g')\n",
    "plt.legend(['Train Loss', 'Validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим параметры обучения:\n",
    "* `epochs` - число эпох. Эпоха заканчивается, когда сеть увидела все примеры тренировочного множества;\n",
    "* `batch_size` - размер мини-батча. В [SGD](http://www.machinelearning.ru/wiki/index.php?title=Стохастический_градиентный_спуск) элементы обучающей выборки разбиваются на группы (мини-батчи) и функция потерь, вместе с её производной, рассчитывается на мини-батчах. Веса обновляются после просмотра каждого мини-батча. Вычислительно выгодней, чтобы мини-батч был достаточно большого размера, поскольку в таком случае данные будут обрабатываться параллельно;\n",
    "* `validation_data` - данные для построения кривой ошибки на валидационных данных \"Validation loss\".\n",
    "\n",
    "По графику функции потерь (loss) можно судить о том, имеет ли смысл обучать сеть большее число итераций.\n",
    "\n",
    "Посмотрим на качество полученной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = r2_score(y_train, dense_model.predict(X_train))\n",
    "test_score  = r2_score(y_test, dense_model.predict(X_test))\n",
    "\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить те же результаты можно за меньшее число эпох обучения с помощью стандартизации входных данных.\n",
    "Так происходит из-за того, что в противном случае сети приходится самой сначала \"искать положение данных\".\n",
    "\n",
    "Сделаем предобработку с помощью `StandardScaler` для векторов признаков и `MinMaxScaler` для целевого.\n",
    "А после каждого полносвязного (`Dense`) слоя поставим `BatchNormalization` - этот механизм стандартизации внутри сети рекомендуется применять почти во всех случаях. Грубо говоря, без него каждый слой сам должен \"искать положение\" своих входных данных, хотя эту операцию можно сделать явно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.layers import BatchNormalization\n",
    "np.random.seed(1)\n",
    "set_random_seed(1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.reshape(y_train.shape[0], 1))\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_test_scaled = target_scaler.transform(y_test.reshape(y_test.shape[0], 1))\n",
    "\n",
    "\n",
    "def make_dense_scaled_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    return model\n",
    "    \n",
    "    \n",
    "dense_scaled_model = make_dense_scaled_model()\n",
    "\n",
    "history = dense_scaled_model.fit(X_train_scaled, y_train_scaled,\n",
    "                                 epochs=n_epochs,\n",
    "                                 batch_size=16,\n",
    "                                 validation_data=(X_test_scaled, y_test_scaled),\n",
    "                                 verbose=False)\n",
    "\n",
    "plt.plot(range(len(history.history['loss'])), history.history['loss'], c='b')\n",
    "plt.plot(range(len(history.history['val_loss'])), history.history['val_loss'], c='g')\n",
    "plt.legend(['Train Loss', 'Validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_predict(model, X, target_scaler=target_scaler):\n",
    "    return target_scaler.inverse_transform(model.predict(X))\n",
    "\n",
    "\n",
    "train_score = r2_score(y_train, scaled_predict(dense_scaled_model, X_train_scaled))\n",
    "test_score  = r2_score(y_test, scaled_predict(dense_scaled_model, X_test_scaled))\n",
    "\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Нейронные сети для обработки изображений\n",
    "\n",
    "#### Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет fashion mnist, содержащий изображения одежды (grayscale 28x28) 10 различных классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "classes = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images  = test_images / 255.0\n",
    "train_vectors = to_categorical(train_labels)\n",
    "test_vectors = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:**\n",
    "\n",
    "Измените параметры свёрточной сети / параметры обучения (метода `fit`), чтобы повысить качество (accuracy) на тестовых данных, желательно до $0.88$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model, Input\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, Reshape, GlobalMaxPooling2D,\n",
    "                          Activation)\n",
    "\n",
    "image_model = Sequential()\n",
    "image_model.add(Reshape((28, 28, 1), input_shape=(28, 28)))\n",
    "# далее input_shape указывать не обязательно\n",
    "# следует изменить параметры следующих слоёв:\n",
    "image_model.add(Conv2D(4, kernel_size=(3, 3), strides=(1, 1),\n",
    "                       input_shape=(28, 28, 1), activation='relu'))\n",
    "image_model.add(BatchNormalization())\n",
    "image_model.add(MaxPooling2D())\n",
    "image_model.add(Conv2D(4, kernel_size=(3, 3), strides=(1, 1),\n",
    "                       input_shape=(14, 14, 4), activation='relu'))\n",
    "image_model.add(BatchNormalization())\n",
    "image_model.add(MaxPooling2D())\n",
    "image_model.add(Conv2D(4, kernel_size=(3, 3), strides=(1, 1),\n",
    "                       input_shape=(7, 7, 4), activation='relu'))\n",
    "image_model.add(BatchNormalization())\n",
    "image_model.add(Flatten())\n",
    "\n",
    "classification_model = Sequential()\n",
    "classification_model.add(image_model)\n",
    "classification_model.add(Dense(4, activation='relu'))\n",
    "classification_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.compile(optimizer='adam', \n",
    "                             loss='categorical_crossentropy',\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "hist = classification_model.fit(train_images, train_vectors, epochs=5, batch_size=128,\n",
    "                                validation_data=(test_images, test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    acc = 'acc' if 'acc' in history else 'accuracy'\n",
    "    val_acc = 'val_' + acc\n",
    "    \n",
    "    plt.plot(range(len(history[acc])), history[acc], color='b')\n",
    "    plt.plot(range(len(history[val_acc])), history[val_acc], color='g')\n",
    "\n",
    "    return hist.history[val_acc][-1]\n",
    "\n",
    "test_acc = plot_accuracy(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Качество: {test_acc}\")\n",
    "print(\"Тест на качество {}\".format(\"не пройден :(\" if 0.88 > test_acc else \"пройден :)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Посмотрим как выглядит предсказание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "test_ind = np.random.randint(test_images.shape[0])\n",
    "test_image = test_images[test_ind]\n",
    "test_vector = test_vectors[test_ind]\n",
    "predicted_test_vector = classification_model.predict(test_image[np.newaxis]).reshape((10,))\n",
    "\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "plt.title('Тестовое изображение')\n",
    "plt.figure()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(17, 4))\n",
    "ax[0].bar(range(10), test_vector, tick_label=classes)\n",
    "ax[1].bar(range(10), predicted_test_vector, tick_label=classes)\n",
    "for i, title in enumerate(['Правильный вектор классов', 'Предсказанный вектор']): ax[i].set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим предсказания для первых 100 изображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "predicted_vectors = classification_model.predict(test_images[:100]).reshape((100, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на каких изображениях сеть ошибается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted_labels = np.argmax(predicted_vectors, axis=1)  # предсказанные индексы классов - где верояность наибольшая\n",
    "\n",
    "pred = test_labels[:100] != predicted_labels\n",
    "misclassified_images = test_images[:100][pred]\n",
    "misclassified_correct = test_labels[:100][pred]\n",
    "misclassified_predicted = predicted_labels[:100][pred]\n",
    "\n",
    "cm = confusion_matrix(test_labels[:100], predicted_labels[:100])\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.xticks(range(10), classes, rotation=90)\n",
    "plt.yticks(range(10), classes)\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.figure()\n",
    "\n",
    "fig, ax = plt.subplots(2, 6, figsize=(15, 4))\n",
    "for i in range(2 * 6):\n",
    "    ax.flatten()[i].imshow(misclassified_images[i], cmap='gray')\n",
    "    ax.flatten()[i].axis('off')\n",
    "    ax.flatten()[i].set_title(f\"{classes[misclassified_predicted[i]]} / {classes[misclassified_correct[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Gradient Boosting для классификации изображений\n",
    "\n",
    "#### Задание 3.\n",
    "\n",
    "Будем рассматривать ту же задачу, что и в прошлом задании.\n",
    "\n",
    "Возьмём вектор признаков, полученных в результате работы уже обученной свёрточной модели `image_model` из предыдущего задания, и достроим классификатор на основе градиентного бустинга.\n",
    "\n",
    "**Требуется:**\n",
    "\n",
    "Изменить параметры `CatBoostClassifier` ниже, чтобы повысить качество (accuracy), желательно до `0.89`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = image_model.predict(train_images)\n",
    "test_encoded = image_model.predict(test_images)\n",
    "\n",
    "dim_reduction = PCA(n_components=36) # понизим размерность, используя информацию о классах\n",
    "small_train_encoded = dim_reduction.fit_transform(train_encoded)\n",
    "small_test_encoded = dim_reduction.transform(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "image_gbm = CatBoostClassifier(loss_function='MultiClass',\n",
    "                               # эти параметры следует подобрать:\n",
    "                               depth=3,\n",
    "                               learning_rate=0.03,\n",
    "                               iterations=500)\n",
    "image_gbm.fit(small_train_encoded, train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "image_gbm_labels = image_gbm.predict(small_test_encoded)\n",
    "image_gbm_score = accuracy_score(test_labels, image_gbm_labels)\n",
    "print(f\"Качество: {image_gbm_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Аугментация\n",
    "\n",
    "Часто доступные на этапе обучения размеченные данные оказываются смещёнными, не целиком покрывающими допустимое для данной задачи распределение входных данных.\n",
    "Также, при использовании нейронных сетей с большим числом параметров, на этапе обучения необходимо большое число различных данных, чтобы модель не деградировала до \"табличной функции\", запоминая все входные данные.\n",
    "\n",
    "Для борьбы с обеими проблемами хорошо подходит подход искусственного расширения обучающего набора данных – аугментация. Идея заключается в применении случайных трансформаций к каждому отдельному изображению, при сохранении меток классов. Таким образом можно получить датасет произвольного размера и добиться от нейронной сети определённых свойств (разумеется, для фиксированного, ограниченного распределения входных данных), например: инвариантности к повороту объекта – за счёт обучения на всевозможных поворотах входных изображений.\n",
    "\n",
    "Важно, что выбор набора трансформаций и распределений из которых генерируются их параметры, специфичен для каждой задачи. Например, при классификации деревьев не разумно выполнять повороты на $\\pi$, поскольку известно, что деревья всегда растут снизу вверх.\n",
    "\n",
    "---\n",
    "\n",
    "Будем загружать датасет из GoogleDisk, установим библиотеку для удобного скачивания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем файлы с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import os\n",
    "\n",
    "ids_to_file_names = {\n",
    "    '1iFK4-fCqs8l5lWmYnUYlkv28GDb43FUv': 'fashion_images.npy',\n",
    "    '15NUcK_Pg4iEUttK1XiG0SonJaEbaaVnI': 'fashion_labels.npy',\n",
    "    '1tWy1d9lkrCzkiSZBlfd1G6YV4sZ7pNt4': 'val_fashion_images.npy',\n",
    "    '1AXtbCz3EE2xVuUrGgEco7a-lMh8Vqm0o': 'val_fashion_labels.npy',\n",
    "    '1fk1mZxBLRryWp4IoLHRr5IwiaKy4WL_Y': 'test_fashion_images.npy',\n",
    "}\n",
    "\n",
    "for file_id in ids_to_file_names:\n",
    "    # Качаем файл только если он ещё не скачан\n",
    "    if not os.path.isfile(ids_to_file_names[file_id]):\n",
    "        gdd.download_file_from_google_drive(file_id=file_id,\n",
    "                                            dest_path=os.path.join('.', ids_to_file_names[file_id]),\n",
    "                                            unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные – изображения 64x64 с цветным шумом и случайно аффинно трансформированными снимками одежды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_images = np.load(\"fashion_images.npy\")\n",
    "fashion_labels = np.load(\"fashion_labels.npy\")\n",
    "\n",
    "val_fashion_images = np.load(\"val_fashion_images.npy\")\n",
    "val_fashion_labels = np.load(\"val_fashion_labels.npy\")\n",
    "\n",
    "test_fashion_images = np.load(\"test_fashion_images.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве тренировочного и валидационного датасета даны изображения и метки (вектора вероятностей классов одежды), в качестве тестового – только изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже дана модель, гиперпараметры которой (количества каналов, размеры ядер, функции активации, архитектуру и т.п.) менять **нельзя**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fashion_model():\n",
    "    imodel = Sequential()\n",
    "    imodel.add(Reshape((64, 64, 3), input_shape=(64, 64, 3)))\n",
    "    imodel.add(Conv2D(4, kernel_size=(3, 3), strides=(1, 1),\n",
    "                         activation='relu', padding='same'))\n",
    "    imodel.add(BatchNormalization())\n",
    "    imodel.add(MaxPooling2D())\n",
    "    imodel.add(Conv2D(8, kernel_size=(3, 3), strides=(1, 1),\n",
    "                         activation='relu', padding='same'))\n",
    "    imodel.add(BatchNormalization())\n",
    "    imodel.add(MaxPooling2D())\n",
    "    imodel.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1),\n",
    "                         activation='relu', padding='same'))\n",
    "    imodel.add(BatchNormalization())\n",
    "    imodel.add(MaxPooling2D())\n",
    "    imodel.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                         activation='relu', padding='same'))\n",
    "    imodel.add(BatchNormalization())\n",
    "    imodel.add(MaxPooling2D())\n",
    "    imodel.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n",
    "                         activation='relu', padding='same'))\n",
    "    imodel.add(BatchNormalization())\n",
    "    imodel.add(MaxPooling2D())\n",
    "    imodel.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                         activation='relu', padding='same'))\n",
    "    imodel.add(BatchNormalization())\n",
    "    imodel.add(Flatten())\n",
    "\n",
    "    iclf = Sequential()\n",
    "    iclf.add(imodel)\n",
    "    iclf.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    iclf.compile(optimizer='adam', \n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return iclf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Задание 4 (дополнительное).\n",
    "\n",
    "**Внимание!** Это задание повышенной сложности, за его решение даётся 3 балла, выполнять его необязательно.\n",
    "\n",
    "Найти такие параметры аугментации (ниже), чтобы качество классификации (accuracy) на тестовых данных было не ниже $0.6$.\n",
    "\n",
    "*Настоятельно рекомендуется сначала внимательно изучить обучающую и тестовую выборки, а не подбирать параметры случайным образом.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fashion_images = fashion_images.copy()\n",
    "# здесь можно сделать предварительную обработку всего обучающего датасета\n",
    "train_fashion_labels = fashion_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# здесь следует изменить параметры аугментации:\n",
    "aug = ImageDataGenerator(featurewise_center=False,\n",
    "                   samplewise_center=False,\n",
    "                   featurewise_std_normalization=False,\n",
    "                   samplewise_std_normalization=False,\n",
    "                   rotation_range=0,\n",
    "                   width_shift_range=0.0,\n",
    "                   height_shift_range=0.0,\n",
    "                   brightness_range=None,\n",
    "                   shear_range=0.0,\n",
    "                   zoom_range=0.0,\n",
    "                   channel_shift_range=0.0,\n",
    "                   fill_mode='nearest',\n",
    "                   cval=0.0,\n",
    "                   horizontal_flip=False,\n",
    "                   vertical_flip=False,\n",
    "                   rescale=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из параметров обучения можно менять `batch_size` и `epochs`, но использовать можно не больше 200 эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "fashion_clf = make_fashion_model()\n",
    "\n",
    "aug.fit(train_fashion_images)\n",
    "gen = aug.flow(train_fashion_images, train_fashion_labels,\n",
    "               batch_size=batch_size)\n",
    "\n",
    "hist = fashion_clf.fit(gen,\n",
    "               steps_per_epoch=len(train_fashion_images) // batch_size,\n",
    "               epochs=200,\n",
    "               validation_data=(val_fashion_images, val_fashion_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(hist.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки сохраняются веса сети, а также предсказания для тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_clf.save(\"fashion_model.h5\")\n",
    "\n",
    "test_predictions = fashion_clf.predict(test_fashion_images)\n",
    "np.save(\"submission\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файлы `fashion_model.h5` и `submission.npy` необходимо добавить в репозиторий вместе с решением.\n",
    "\n",
    "Любые попытки получить более точную модель не с помощью аугментации запрещены.\n",
    "\n",
    "---\n",
    "\n",
    "## Выбор подходящей модели машинного обучения\n",
    "\n",
    "Возьмём набор данных `wine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine_x, wine_y = load_wine(return_X_y=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wine_x, wine_y, test_size=0.2, random_state=123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Задание 5 (дополнительное).\n",
    "\n",
    "Подберите модель машинного обучения для решения задачи _регрессии_ на данных из датасета `wine`.\n",
    "\n",
    "Обучите выбранную модель, используя для обучения только данные `x_train`, `y_train`.\n",
    "\n",
    "Данные `x_test`, `y_test` можно использовать в обучении _только в качестве валидационных данных_.\n",
    "\n",
    "Метрика MAE (mean absolute error, средняя квадратическая ошибка) на тестовых данных (`x_test`, `y_test`) должна быть меньше `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код обучения модели\n",
    "wine_regressor = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(f\"MAE on test data = {mean_absolute_error(y_test, wine_regressor.predict(x_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
